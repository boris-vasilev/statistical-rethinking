# 9.3 Hamiltoninan Monte Carlo

*When there's a randomised way of doing something, there usually is a non-randomised way that has better performance but requires more thought*

Gibbs is already such an improvement over Metropolis. It's more efficient by using proposals that are not purely random but based on knowledge of the target distribution.

**Hamiltonian Monte Carlo (HMC)** is another step up. It's more computatinally costly than Gibbs, but its proposals are much more efficient. It takes less time overall even though it takes more time per step.

## 9.3.1 Another parable

The king now traverses not islands but a mainland region with hills.

The population is inversely proportional to the altitude.

The king again has to visit everyone proportionally to the population - so the higher it is the less time his spends.

1.  He starts in a random direction, at a random momentum (step size)
2.  Going uphill reduces momentum, going downhill increases momentum. Eventually going uphill can negate the initial momentum and reverse the king's direction.
3.  At regular times the king stops (sampling)

The **autocorrelation** between locations visited using that strategy is very low. Adjacent locations have a very low, almost zero correlation.

To contrast, in Gibbs sampling because we had proposals close to the current sample when we get stuck in a concentration of measure valley, the autocorrelation was very high, the parameter values were very similar.

This strategy however works only for continuous distributions.

The islands in the first example are not continuous.

## 9.3.2 Particles in space

HMC uses a current vector of parameter values (the vehicle from the king's metaphor)

HMC runs a physics simulation pretending the vector of parameters gives the position of a frictionless particle.

The log-posterior is the surface the particle glides on.

When the surface is very flat because the priors are uniformative, or there isn't much information in the likelihood, the particle can glide for a long time before changing direction.

When the log-posterior is very steep, because the likelihood or the priors are very concentrated, it doesn't get very far before turning around.

HMC always accepts every proposal because it makes only intelligent proposals.

HMC uses a rejection criterion. Because HMC is running a physics simulation, certain things have to be considered like total energy of the system. When total energy changes it means that the numerical approximation is bad and the proposal might get rejected.

Let's see how HMC approximates a 2D Gaussian.

Suppose we have **data**: **100 x and 100 y values sampled from** $Normal(0, 1)$ and we have this **model**:

$$
x_i \sim Normal(\mu_x, 1) \\
y_i \sim Normal(\mu_y, 1) \\
\mu_x \sim Normal(0, 0.5) \\
\mu_y \sim Normal(0, 0.5) \\
$$

HMC needs two functions:

-   function that computes log-probability of the data and parameters $$
    \sum_i{log(p(y_i | \mu_y, 1)} + \sum_i{log(p(x_i | \mu_x, 1)} + log(p(\mu_y | 0, 0.5) + log(p(\mu_x | 0, 0.5)
    $$\
    This is just the top part of Bayes' formula, and every MCMC strategy requires it. It tells the "elevation" of any set of parameter values.

-   It also needs the **gradient**, which means the slope in any direction at the current position

    In this case this is two derivatives.

    If we take the expression above and differentiate it with respect to $\mu_x$ and $\mu_y$ we get that

HMC also needs two settings:

-   Number of **leapfrog steps**

-   **Step size**

Each path between visits (samples) is made up of leapfrog steps. So the higher the number of leapfrog steps the longer the path is. The step size controls how big those steps are.

The steps determine how fine-grained the simulation would be. The bigger it is, the sharper the turns would be. If it's too large it can also overshoot.

Those are usually automatically picked by the computer.

```{r}
## R code 9.5
# U needs to return neg-log-probability
U <- function( q , a=0 , b=1 , k=0 , d=1 ) {
    muy <- q[1]
    mux <- q[2]
    U <- sum( dnorm(y,muy,1,log=TRUE) ) + sum( dnorm(x,mux,1,log=TRUE) ) +
        dnorm(muy,a,b,log=TRUE) + dnorm(mux,k,d,log=TRUE)
    return( -U )
}

## R code 9.6
# gradient function
# need vector of partial derivatives of U with respect to vector q
U_gradient <- function( q , a=0 , b=1 , k=0 , d=1 ) {
    muy <- q[1]
    mux <- q[2]
    G1 <- sum( y - muy ) + (a - muy)/b^2 #dU/dmuy
    G2 <- sum( x - mux ) + (k - mux)/d^2 #dU/dmux
    return( c( -G1 , -G2 ) ) # negative bc energy is neg-log-prob
}
# test data
set.seed(7)
y <- rnorm(50)
x <- rnorm(50)
x <- as.numeric(scale(x))
y <- as.numeric(scale(y))

## R code 9.7
library(shape) # for fancy arrows
Q <- list()
Q$q <- c(-0.1,0.2)
pr <- 0.3
plot( NULL , ylab="muy" , xlab="mux" , xlim=c(-pr,pr) , ylim=c(-pr,pr) )
step <- 0.03
L <- 11 # 0.03/28 for U-turns --- 11 for working example
n_samples <- 4
path_col <- col.alpha("black",0.5)
points( Q$q[1] , Q$q[2] , pch=4 , col="black" )
for ( i in 1:n_samples ) {
    Q <- HMC2( U , U_gradient , step , L , Q$q )
    if ( n_samples < 10 ) {
      for ( j in 1:L ) {
        K0 <- sum(Q$ptraj[j,]^2)/2 # kinetic energy
        lines( Q$traj[j:(j+1),1] , Q$traj[j:(j+1),2] , col=path_col , lwd=1+2*K0 )
      }
      points( Q$traj[1:L+1,] , pch=16 , col="white" , cex=0.35 )
      Arrows( Q$traj[L,1] , Q$traj[L,2] , Q$traj[L+1,1] , Q$traj[L+1,2] ,
          arr.length=0.35 , arr.adj = 0.7 )
      text( Q$traj[L+1,1] , Q$traj[L+1,2] , i , cex=0.8 , pos=4 , offset=0.4 )
    }
    points( Q$traj[L+1,1] , Q$traj[L+1,2] , pch=ifelse( Q$accept==1 , 16 , 1 ) ,
        col=ifelse( abs(Q$dH)>0.1 , "red" , "black" ) )
}

## R code 9.8
HMC2 <- function (U, grad_U, epsilon, L, current_q) {
  q = current_q
  p = rnorm(length(q),0,1) # random flick - p is momentum.
  current_p = p
  # Make a half step for momentum at the beginning
  p = p - epsilon * grad_U(q) / 2
  # initialize bookkeeping - saves trajectory
  qtraj <- matrix(NA,nrow=L+1,ncol=length(q))
  ptraj <- qtraj
  qtraj[1,] <- current_q
  ptraj[1,] <- p

## R code 9.9
  # Alternate full steps for position and momentum
  for ( i in 1:L ) {
    q = q + epsilon * p # Full step for the position
    # Make a full step for the momentum, except at end of trajectory
    if ( i!=L ) {
        p = p - epsilon * grad_U(q)
        ptraj[i+1,] <- p
    }
    qtraj[i+1,] <- q
  }

## R code 9.10
  # Make a half step for momentum at the end
  p = p - epsilon * grad_U(q) / 2
  ptraj[L+1,] <- p
  # Negate momentum at end of trajectory to make the proposal symmetric
  p = -p
  # Evaluate potential and kinetic energies at start and end of trajectory
  current_U = U(current_q)
  current_K = sum(current_p^2) / 2
  proposed_U = U(q)
  proposed_K = sum(p^2) / 2
  # Accept or reject the state at end of trajectory, returning either
  # the position at the end of the trajectory or the initial position
  accept <- 0
  if (runif(1) < exp(current_U-proposed_U+current_K-proposed_K)) {
    new_q <- q  # accept
    accept <- 1
  } else new_q <- current_q  # reject
  return(list( q=new_q, traj=qtraj, ptraj=ptraj, accept=accept ))
}
```

If the parameters are not tuned well the path may end up turning on itself ending up in the same neighbourhood, which ends up having the same autocorrelation problem GIbbs sampling had (this problem in HMC is known as the **U-turn problem**)

HMC samplers like Stan and rstan deal with the U-turns in the **warmup phase** in which they choose number of leapfrog steps and step size for you that would efficiently explore the posterior.

When you first run Stan it's usually slow in that warmup phase and speeds up later on.

Stan uses a second-generation NUTS2 sampler (**NUTS - No-U-turn sampler**)

## 9.3.3 Limitations

HMC requires continuous parameters.

Some distributions are hard to sample from, for any algorithm. In such cases, HMC will encounter **divergent transition** - when the energy at the start of the trajectory differs substantially from the energy at the end.
